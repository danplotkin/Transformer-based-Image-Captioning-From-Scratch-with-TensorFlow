# Transformer-based-Image-Captioning-From-Scratch-with-TensorFlow

When it comes to image captioning, it has become increasingly popular to use pre-trained transformers such as BERT and GPT-2. These models are highly effective at generating text based on input images, and have been used in a wide range of applications, from creating captions for social media posts to generating product descriptions for online retailers.

In this project, I will demonstrate how to build an image captioner transformer using TensorFlow from scratch. This project will provide a detailed guide on how to design and implement a transformer architecture for image captioning, including how to preprocess the image and text data, train the model, and evaluate its performance.

I will be using the [Flickr 8K Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) for this project.

[Link to Notebook](https://github.com/danplotkin/Transformer-based-Image-Captioning-From-Scratch-with-TensorFlow/blob/main/Image%20Captioning.ipynb)
