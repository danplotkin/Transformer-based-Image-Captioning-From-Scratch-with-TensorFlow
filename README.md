# Transformer-Based-Image-Captioning-From-Scratch-with-TensorFlow

When it comes to image captioning, it has become increasingly popular to use pre-trained transformers such as BERT and GPT-2. They're incredibly effective at generating text based on input images, and have found use in a variety of applications - from creating captions for social media posts to generating product descriptions for online retailers.

In this project, I'm going to show you how to create your very own image captioner transformer from scratch using TensorFlow. I'll guide you through every step of the process - from designing and implementing the transformer architecture for image captioning, to preprocessing the image and text data, training the model, and evaluating its performance. For this project, we'll be using the [Flickr 8K Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) to train our model.

[Link to Notebook](https://github.com/danplotkin/Transformer-based-Image-Captioning-From-Scratch-with-TensorFlow/blob/main/Image%20Captioning.ipynb)
